{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71933042-2ba8-4f21-9f29-d5e0aa07922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#В домашней работе я разобрался, как работают базовые модели LinearRegression, LogisticRegression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeb070b4-a848-41dc-bddf-15af5aebd2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Помимо этого сделал тесты логистической модели для нашего-датасета. Существовала проблема в датасете - дисбаланс классов\n",
    "#~134000 True/ ~3000 False с помощью метода undersampling(реализовал тоже без сторонних библиотек). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9ee2308-c9fe-4a43-a05c-1c29ade5883e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ВАЖНО! В блокноте можно встретить импорт sklearn, но он тут исключительно в целях реализации кросс-валидации и никак не задействован в создании ТЗ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5573ab9f-6c68-4357-9d81-edca767b9660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 1, MSE: 69.0000\n",
      "Эпоха 100, MSE: 0.1855\n",
      "Эпоха 200, MSE: 0.1315\n",
      "Эпоха 300, MSE: 0.0932\n",
      "Эпоха 400, MSE: 0.0661\n",
      "Эпоха 500, MSE: 0.0469\n",
      "Эпоха 600, MSE: 0.0332\n",
      "Эпоха 700, MSE: 0.0236\n",
      "Эпоха 800, MSE: 0.0167\n",
      "Эпоха 900, MSE: 0.0118\n",
      "Эпоха 1000, MSE: 0.0084\n",
      "Предсказания: [ 4.85405315  6.93135412  9.00865508 11.08595604]\n"
     ]
    }
   ],
   "source": [
    "# Начнем с классической модели линейной регрессии.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, learning_rate=0.01, epochs=1000):\n",
    "        \"\"\"\n",
    "        Инициализация параметров модели.\n",
    "\n",
    "        learning_rate — скорость обучения,\n",
    "        epochs — количество итераций.\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.coefficients = None  # Коэффициенты модели (включая свободный член)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Обучение модели методом градиентного спуска.\n",
    "\n",
    "        X — матрица признаков (numpy array или pandas DataFrame),\n",
    "        y — целевая переменная (numpy array или pandas Series).\n",
    "        \"\"\"\n",
    "        X = np.array(X)  # Преобразуем X в numpy array, если это DataFrame\n",
    "        y = np.array(y)  # Преобразуем y в numpy array, если это Series\n",
    "\n",
    "        # Добавляем столбец единиц для свободного члена\n",
    "        X = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "        # Инициализируем коэффициенты нулями\n",
    "        self.coefficients = np.zeros(X.shape[1])\n",
    "\n",
    "        # Количество примеров\n",
    "        m = len(y)\n",
    "\n",
    "        # Обучение с использованием градиентного спуска\n",
    "        for epoch in range(self.epochs):\n",
    "            predictions = X @ self.coefficients  # Предсказания текущей модели\n",
    "            errors = predictions - y  # Ошибки предсказания\n",
    "            gradient = (1 / m) * X.T @ errors  # Градиент\n",
    "            self.coefficients -= self.learning_rate * gradient  # Обновляем коэффициенты\n",
    "\n",
    "            # Для наблюдения: считаем текущий MSE и выводим каждые 100 эпох\n",
    "            mse = np.mean(errors ** 2)\n",
    "            if (epoch + 1) % 100 == 0 or epoch == 0:\n",
    "                print(f\"Эпоха {epoch + 1}, MSE: {mse:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Предсказание на основе обученной модели.\n",
    "\n",
    "        X — матрица признаков (numpy array или pandas DataFrame).\n",
    "        \"\"\"\n",
    "        X = np.array(X)  # Преобразуем X в numpy array, если это DataFrame\n",
    "        X = np.c_[np.ones(X.shape[0]), X]  # Добавляем столбец единиц\n",
    "        return X @ self.coefficients  # Линейное уравнение\n",
    "\n",
    "\n",
    "# Пример данных\n",
    "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])  # Матрица признаков\n",
    "y = np.array([5, 7, 9, 11])  # Целевая переменная\n",
    "\n",
    "# Создаем объект модели линейной регрессии\n",
    "model = LinearRegression(learning_rate=0.01, epochs=1000)\n",
    "\n",
    "# Обучаем модель\n",
    "model.fit(X, y)\n",
    "\n",
    "# Получаем предсказания\n",
    "predictions = model.predict(X)\n",
    "print(\"Предсказания:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89550b4a-f0cc-4c9e-8d71-f3ffee57c982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Так же продемонстрирую классическую реализацию логистической регрессии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9053f38a-64f7-47e9-b5ba-b3aca6d13f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0\n",
      "Recall: 0.8888888888888888\n",
      "F1 Score: 0.9411764705882353\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Сигмоида для преобразования значений.\"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def initialize_parameters(self, n_features):\n",
    "        \"\"\"Инициализация параметров модели.\"\"\"\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Обучение модели с использованием градиентного спуска.\"\"\"\n",
    "        num_samples, num_features = X.shape\n",
    "        self.initialize_parameters(num_features)\n",
    "\n",
    "        for _ in range(self.num_iterations):\n",
    "            # Прогнозы\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = self.sigmoid(linear_model)\n",
    "\n",
    "            # log-loss\n",
    "            loss = -np.mean(y * np.log(y_predicted + 1e-15) + (1 - y) * np.log(1 - y_predicted + 1e-15))\n",
    "\n",
    "            # Градиенты\n",
    "            dw = (1 / num_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / num_samples) * np.sum(y_predicted - y)\n",
    "\n",
    "            # Обновление параметров\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"Прогнозирование меток.\"\"\"\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self.sigmoid(linear_model)\n",
    "        return (y_predicted >= threshold).astype(int)\n",
    "\n",
    "    def evaluate(self, y_true, y_pred):\n",
    "        \"\"\"Оценка метрик precision, recall, f1-score.\"\"\"\n",
    "        tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "        fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "        fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        return {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1_score\": f1_score\n",
    "        }\n",
    "\n",
    "# Пример использования\n",
    "if __name__ == \"__main__\":\n",
    "    # Генерация данных\n",
    "    np.random.seed(42)\n",
    "    X = np.random.rand(100, 2)  # 100 образцов, 2 признака\n",
    "    y = (np.sum(X, axis=1) > 1).astype(int)  # Простое разделение на классы\n",
    "\n",
    "    # Разделение на тренировочные и тестовые данные\n",
    "    split_idx = int(0.8 * len(X))\n",
    "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "    # Создание и обучение модели\n",
    "    model = LogisticRegression(learning_rate=0.1, num_iterations=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Прогнозы\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Оценка метрик\n",
    "    metrics = model.evaluate(y_test, y_pred)\n",
    "    print(\"Precision:\", metrics[\"precision\"])\n",
    "    print(\"Recall:\", metrics[\"recall\"])\n",
    "    print(\"F1 Score:\", metrics[\"f1_score\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09ff551a-68e7-4c33-addb-0d562ace5683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold Results: Precision=0.98, Recall=0.80, F1=0.88\n",
      "Fold Results: Precision=0.98, Recall=0.81, F1=0.88\n",
      "Fold Results: Precision=0.97, Recall=0.80, F1=0.88\n",
      "Fold Results: Precision=0.98, Recall=0.84, F1=0.90\n",
      "Fold Results: Precision=0.98, Recall=0.82, F1=0.90\n",
      "\n",
      "Average Metrics from Cross-Validation:\n",
      "Precision: 0.98\n",
      "Recall: 0.81\n",
      "F1 Score: 0.89\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Перейдем к самой интересной части. Это реализация написана для нашего data-сета. Наша задача бинарной классификации, поэтому я решил использовать\n",
    "модель логистической регрессии. Из проблемных моментов: в датасете к сожалению была найдена проблема дисбаланса классов. Задача решалась простым \n",
    "undersampling-ом. \n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "class CustomLogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        # Инициализация параметров модели\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Сигмоидная функция.\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Обучение модели с использованием градиентного спуска.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)  # Инициализация весов\n",
    "        self.bias = 0  # Инициализация смещения\n",
    "\n",
    "        for _ in range(self.n_iterations):\n",
    "            # Линейная комбинация\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = self.sigmoid(linear_model)\n",
    "\n",
    "            # Градиенты\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "\n",
    "            # Обновление параметров\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Предсказание вероятностей и классификация.\n",
    "        \"\"\"\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self.sigmoid(linear_model)\n",
    "        return np.array([1 if i > 0.5 else 0 for i in y_predicted])\n",
    "\n",
    "    def evaluate(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Оценка метрик: precision, recall, F1.\n",
    "        \"\"\"\n",
    "        # Метрики\n",
    "        tp = np.sum((y_true == 1) & (y_pred == 1))  # True positives\n",
    "        tn = np.sum((y_true == 0) & (y_pred == 0))  # True negatives\n",
    "        fp = np.sum((y_true == 0) & (y_pred == 1))  # False positives\n",
    "        fn = np.sum((y_true == 1) & (y_pred == 0))  # False negatives\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        return precision, recall, f1_score\n",
    "\n",
    "\n",
    "# Загрузка данных\n",
    "data = pd.read_csv('additional_variable_train.csv', delimiter=';', skipinitialspace=True)\n",
    "\n",
    "\n",
    "# Выбор признаков и целевой переменной\n",
    "X = data[['Air temperature [K]', 'Process temperature [K]', 'Rotational speed [rpm]',\n",
    "          'Torque [Nm]', 'Tool wear [min]', 'Sum_Parametr', 'Ratio']].values\n",
    "y = data['Machine failure'].values\n",
    "\n",
    "# Стандартизация данных\n",
    "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "# Балансировка классов методом undersampling\n",
    "class_0_indices = np.where(y == 0)[0]\n",
    "class_1_indices = np.where(y == 1)[0]\n",
    "balanced_class_0_indices = np.random.choice(class_0_indices, size=len(class_1_indices), replace=False)\n",
    "balanced_indices = np.concatenate([balanced_class_0_indices, class_1_indices])\n",
    "np.random.shuffle(balanced_indices)\n",
    "\n",
    "X_balanced = X[balanced_indices]\n",
    "y_balanced = y[balanced_indices]\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Метрики\n",
    "metrics = {'precision': [], 'recall': [], 'f1_score': []}\n",
    "\n",
    "# Создание модели\n",
    "model = CustomLogisticRegression(learning_rate=0.01, n_iterations=10000)\n",
    "\n",
    "# Кросс-валидация\n",
    "for train_index, test_index in kf.split(X_balanced):\n",
    "    X_train, X_test = X_balanced[train_index], X_balanced[test_index]\n",
    "    y_train, y_test = y_balanced[train_index], y_balanced[test_index]\n",
    "\n",
    "    # Обучение\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Предсказания\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    # Оценка метрик\n",
    "    precision, recall, f1 = model.evaluate(y_test, predictions)\n",
    "\n",
    "    metrics['precision'].append(precision)\n",
    "    metrics['recall'].append(recall)\n",
    "    metrics['f1_score'].append(f1)\n",
    "\n",
    "    # Вывод метрик для каждого фолда\n",
    "    print(f\"Fold Results: Precision={precision:.2f}, Recall={recall:.2f}, F1={f1:.2f}\")\n",
    "\n",
    "# Итоговые метрики\n",
    "print(\"\\nAverage Metrics from Cross-Validation:\")\n",
    "print(f\"Precision: {np.mean(metrics['precision']):.2f}\")\n",
    "print(f\"Recall: {np.mean(metrics['recall']):.2f}\")\n",
    "print(f\"F1 Score: {np.mean(metrics['f1_score']):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53733773-140a-4a19-a27e-076796bec1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Я пробовал реализовать это и через sklearn. Результаты метрик получил очень схожие."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38d2beff-28a9-4771-9fb0-d3cec98eea73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in train_data: Index(['id', 'Air temperature [K]', 'Process temperature [K]',\n",
      "       'Rotational speed [rpm]', 'Torque [Nm]', 'Ratio',\n",
      "       'Deviation from Average', 'Tool wear [min]', 'Machine failure',\n",
      "       'Tool Wear Failure [TWF]', 'Heat Dissipation Failure [HDF]',\n",
      "       'Power Failure [PWF]', 'Overstrain Failure [OSF]',\n",
      "       'Random Failure [RNF]', 'Sum_Parametr', 'Type_H', 'Type_L', 'Type_M'],\n",
      "      dtype='object')\n",
      "Class distribution in training data: {np.int64(0): np.int64(134281), np.int64(1): np.int64(2148)}\n",
      "Class distribution in balanced training data: {np.int64(0): np.int64(134281), np.int64(1): np.int64(134281)}\n",
      "Cross-validated accuracy (Logistic Regression): 0.8999709564271937\n",
      "\n",
      "Classification Report (Cross-Validation):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.91    134281\n",
      "           1       0.96      0.84      0.89    134281\n",
      "\n",
      "    accuracy                           0.90    268562\n",
      "   macro avg       0.91      0.90      0.90    268562\n",
      "weighted avg       0.91      0.90      0.90    268562\n",
      "\n",
      "\n",
      "Confusion Matrix (Cross-Validation):\n",
      " [[129435   4846]\n",
      " [ 22018 112263]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Загрузка данных для обучения\n",
    "train_data = pd.read_csv('additional_variable_train.csv', delimiter=';', skipinitialspace=True)\n",
    "print(\"Columns in train_data:\", train_data.columns)\n",
    "\n",
    "# Подготовка данных для обучения\n",
    "X = train_data[['Air temperature [K]', 'Process temperature [K]', 'Rotational speed [rpm]',\n",
    "                'Torque [Nm]', 'Tool wear [min]', 'Sum_Parametr', 'Ratio']].values\n",
    "y = train_data['Machine failure'].values\n",
    "\n",
    "# Проверка дисбаланса классов\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print(\"Class distribution in training data:\", dict(zip(unique, counts)))\n",
    "\n",
    "# Стандартизация данных\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Балансировка данных с помощью SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "\n",
    "# Проверка распределения классов после балансировки\n",
    "unique_balanced, counts_balanced = np.unique(y_balanced, return_counts=True)\n",
    "print(\"Class distribution in balanced training data:\", dict(zip(unique_balanced, counts_balanced)))\n",
    "\n",
    "# Создаем модель логистической регрессии\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Применяем кросс-валидацию для предсказания на всех данных\n",
    "# Используем 5-кратную кросс-валидацию\n",
    "predictions = cross_val_predict(model, X_balanced, y_balanced, cv=5)\n",
    "\n",
    "# Оценка модели\n",
    "accuracy = accuracy_score(y_balanced, predictions)\n",
    "print(\"Cross-validated accuracy (Logistic Regression):\", accuracy)\n",
    "print(\"\\nClassification Report (Cross-Validation):\\n\", classification_report(y_balanced, predictions))\n",
    "print(\"\\nConfusion Matrix (Cross-Validation):\\n\", confusion_matrix(y_balanced, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69e473c-0fb6-4534-bdc1-e05dcf7a3dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
