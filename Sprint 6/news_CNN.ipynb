{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oxGTUqar3oLw",
    "outputId": "04ccc38a-5e40-44eb-9553-2ebf105bc421"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
      "Requirement already satisfied: torchtext==0.5.0 in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (2.0.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.5.0) (4.66.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.5.0) (2.32.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.5.0) (1.26.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.5.0) (1.16.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from torchtext==0.5.0) (0.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (75.1.0)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.45.1)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (3.30.5)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (18.1.8)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.5.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.5.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.5.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.5.0) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.0.1 torchtext==0.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XGFt-_KSfd7O"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EknqG4PejCPo",
    "outputId": "fac25abb-4152-4b63-e405-73d0cc42737b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4952\n"
     ]
    }
   ],
   "source": [
    "categories = ['sci.med','sci.electronics', 'sci.space', 'rec.sport.baseball', 'soc.religion.christian']\n",
    "\n",
    "news_data = fetch_20newsgroups(subset='all', categories=categories)\n",
    "\n",
    "# Создание обучающей и тестовой выборок\n",
    "X, y = news_data.data, news_data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(len(news_data.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uTXIIPzmOOCH",
    "outputId": "de71cec9-fd0a-45c3-9650-4c867e64c494"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Удаление метаданных, токенизация и лемматизация\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"(^From|Subject|Organization|Lines):.*\", \"\", text, flags=re.MULTILINE)\n",
    "    text = re.sub(r\"\\b(?:http|https|www)\\S+\\b\", \"\", text)  # Удаляем URL\n",
    "    text = re.sub(r\"\\W\", \" \", text)  # Удаление знаков препинания\n",
    "    text = re.sub(r\"\\d+\", \" \", text)  # Удаление чисел\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip().lower()  # Приведение к нижнему регистру\n",
    "    return text\n",
    "\n",
    "# Загрузка необходимых ресурсов NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Токенизация\n",
    "    tokens = word_tokenize(text)\n",
    "    # Удаление стоп-слов и лемматизация\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Применяем очистку и предобработку\n",
    "X_train_cleaned = [preprocess_text(clean_text(doc)) for doc in X_train]\n",
    "X_test_cleaned = [preprocess_text(clean_text(doc)) for doc in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "yPHKkOzoOaQy"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Создаем словарь из тренировочных данных\n",
    "all_tokens = [word for doc in X_train_cleaned for word in doc.split()]\n",
    "vocab = {word: idx for idx, (word, _) in enumerate(Counter(all_tokens).most_common(), start=1)}\n",
    "\n",
    "# Добавляем новые два ключа в словарь - специальные токены '<unk'> и '<pad>'.\n",
    "vocab[\"<unk>\"] = 0  # Для неизвестных слов\n",
    "vocab[\"<pad>\"] = len(vocab)  # Для паддинга\n",
    "\n",
    "# Преобразуем текст в последовательности индексов\n",
    "def text_to_sequence(text, vocab):\n",
    "    return [vocab.get(word, vocab[\"<unk>\"]) for word in text.split()]\n",
    "\n",
    "X_train_seq = [text_to_sequence(doc, vocab) for doc in X_train_cleaned]\n",
    "X_test_seq = [text_to_sequence(doc, vocab) for doc in X_test_cleaned]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRl9PCItfTZ9"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "IvK7IJ7nOb8Q"
   },
   "outputs": [],
   "source": [
    "# Функция для выравнивания длины последовательностей\n",
    "def pad_sequences(sequences, vocab, max_len=None):\n",
    "    max_len = max_len or max(len(seq) for seq in sequences)\n",
    "    padded = [seq + [vocab[\"<pad>\"]] * (max_len - len(seq)) for seq in sequences]\n",
    "    return torch.tensor(padded, dtype=torch.long)\n",
    "\n",
    "# Паддинг для тренировочных и тестовых данных\n",
    "X_train_padded = pad_sequences(X_train_seq, vocab)\n",
    "X_test_padded = pad_sequences(X_test_seq, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9t44EimvkT9R"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train_padded, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "def shorten_sequences(data, max_len):\n",
    "    return [seq[:max_len] for seq in data]\n",
    "\n",
    "X_train_small = shorten_sequences(X_train_split, max_len=400)\n",
    "X_val_small = shorten_sequences(X_val_split, max_len=400)\n",
    "\n",
    "# Создание датасетов\n",
    "train_dataset = TextDataset(X_train_small, y_train_split)\n",
    "val_dataset = TextDataset(X_val_small, y_val_split)\n",
    "test_dataset = TextDataset(X_test_padded, y_test)\n",
    "\n",
    "# DataLoader для тренировочных, валидационных и тестовых данных\n",
    "train_loader = DataLoader(train_dataset, batch_size=2048, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2048, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "W02-q5Xc3k10"
   },
   "outputs": [],
   "source": [
    "# Создание матрицы эмбеддингов с использованием GloVe из Torchtext\n",
    "def create_embedding_matrix(vocab, glove, embedding_dim=100):\n",
    "    vocab_size = len(vocab)\n",
    "    embedding_matrix = np.random.normal(size=(vocab_size, embedding_dim))  # Для неизвестных слов\n",
    "\n",
    "    for word, idx in vocab.items():\n",
    "        if word in glove.stoi:\n",
    "            embedding_matrix[idx] = glove[word].numpy()  # Получение вектора для слова\n",
    "        else:\n",
    "            embedding_matrix[idx] = np.random.normal(size=(embedding_dim,))\n",
    "\n",
    "    return torch.tensor(embedding_matrix, dtype=torch.float)\n",
    "\n",
    "# Загрузка эмбеддингов GloVe\n",
    "glove = GloVe(name='6B', dim=100)\n",
    "\n",
    "embedding_dim = 100\n",
    "embedding_matrix = create_embedding_matrix(vocab, glove, embedding_dim)\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout, embedding_matrix):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim) # Инициализация матрицы эмбеддингов\n",
    "        self.embedding.weight.data.copy_(embedding_matrix)  # Предобученные веса\n",
    "        self.embedding.weight.requires_grad = False  # Предобученные эмбеддинги не будут изменяться\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(fs, embedding_dim))\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim) # Выходной слой\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        conved = [F.relu(conv(embedded).squeeze(3)) for conv in self.convs]\n",
    "        pooled = [F.max_pool1d(c, c.shape[2]).squeeze(2) for c in conved]\n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1NaehE4hkNX-",
    "outputId": "9ba2b1b0-b0a2-444a-f013-c64bc5c63f84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (embedding): Embedding(35338, 100)\n",
      "  (convs): ModuleList(\n",
      "    (0): Conv2d(1, 120, kernel_size=(2, 100), stride=(1, 1))\n",
      "    (1): Conv2d(1, 120, kernel_size=(3, 100), stride=(1, 1))\n",
      "    (2): Conv2d(1, 120, kernel_size=(4, 100), stride=(1, 1))\n",
      "    (3): Conv2d(1, 120, kernel_size=(5, 100), stride=(1, 1))\n",
      "  )\n",
      "  (fc): Linear(in_features=480, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "n_filters = 120\n",
    "filter_sizes = [2, 3, 4, 5] # Рассматриваются последовательности из 2, 3, 4 и 5 слов\n",
    "output_dim = len(set(y_train)) # Количество классов\n",
    "dropout = 0.5\n",
    "\n",
    "# Инициализация модели\n",
    "model = CNN(vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout, embedding_matrix)\n",
    "# Инициализация оптимизатора\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(model)\n",
    "\n",
    "# Аппаратные параметры для обучения\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h4IKbv70unni",
    "outputId": "e00e6433-b262-4359-c6de-af9f483a5114"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train Loss: 1.750, Train Acc: 20.82%, Train F1: 0.163, Val. Loss: 1.561, Val. Acc: 37.75%, Val. F1: 0.350, Current learning rate: 0.000875\n",
      "Epoch: 02, Train Loss: 1.665, Train Acc: 24.13%, Train F1: 0.237, Val. Loss: 1.514, Val. Acc: 41.07%, Val. F1: 0.316, Current learning rate: 0.002207\n",
      "Epoch: 03, Train Loss: 1.537, Train Acc: 31.60%, Train F1: 0.289, Val. Loss: 1.296, Val. Acc: 64.41%, Val. F1: 0.647, Current learning rate: 0.004132\n",
      "Epoch: 04, Train Loss: 1.273, Train Acc: 49.17%, Train F1: 0.496, Val. Loss: 1.018, Val. Acc: 82.28%, Val. F1: 0.817, Current learning rate: 0.006268\n",
      "Epoch: 05, Train Loss: 0.972, Train Acc: 69.37%, Train F1: 0.685, Val. Loss: 0.686, Val. Acc: 89.19%, Val. F1: 0.893, Current learning rate: 0.008193\n",
      "Epoch: 06, Train Loss: 0.646, Train Acc: 81.53%, Train F1: 0.816, Val. Loss: 0.424, Val. Acc: 91.93%, Val. F1: 0.919, Current learning rate: 0.009525\n",
      "Epoch: 07, Train Loss: 0.419, Train Acc: 88.24%, Train F1: 0.882, Val. Loss: 0.310, Val. Acc: 91.79%, Val. F1: 0.917, Current learning rate: 0.010000\n",
      "Epoch: 08, Train Loss: 0.284, Train Acc: 90.91%, Train F1: 0.909, Val. Loss: 0.242, Val. Acc: 92.80%, Val. F1: 0.929, Current learning rate: 0.009920\n",
      "Epoch: 09, Train Loss: 0.233, Train Acc: 92.64%, Train F1: 0.927, Val. Loss: 0.213, Val. Acc: 92.80%, Val. F1: 0.928, Current learning rate: 0.009681\n",
      "Epoch: 10, Train Loss: 0.173, Train Acc: 94.30%, Train F1: 0.943, Val. Loss: 0.200, Val. Acc: 93.23%, Val. F1: 0.932, Current learning rate: 0.009292\n",
      "Epoch: 11, Train Loss: 0.145, Train Acc: 95.02%, Train F1: 0.950, Val. Loss: 0.189, Val. Acc: 94.09%, Val. F1: 0.941, Current learning rate: 0.008765\n",
      "Epoch: 12, Train Loss: 0.119, Train Acc: 96.14%, Train F1: 0.961, Val. Loss: 0.183, Val. Acc: 93.66%, Val. F1: 0.937, Current learning rate: 0.008117\n",
      "Epoch: 13, Train Loss: 0.108, Train Acc: 96.61%, Train F1: 0.966, Val. Loss: 0.186, Val. Acc: 94.09%, Val. F1: 0.941, Current learning rate: 0.007369\n",
      "Epoch: 14, Train Loss: 0.095, Train Acc: 97.44%, Train F1: 0.974, Val. Loss: 0.185, Val. Acc: 93.95%, Val. F1: 0.939, Current learning rate: 0.006545\n",
      "Epoch: 15, Train Loss: 0.070, Train Acc: 97.80%, Train F1: 0.978, Val. Loss: 0.176, Val. Acc: 94.38%, Val. F1: 0.944, Current learning rate: 0.005671\n",
      "Epoch: 16, Train Loss: 0.073, Train Acc: 97.87%, Train F1: 0.979, Val. Loss: 0.171, Val. Acc: 94.09%, Val. F1: 0.941, Current learning rate: 0.004776\n",
      "Epoch: 17, Train Loss: 0.061, Train Acc: 98.52%, Train F1: 0.985, Val. Loss: 0.168, Val. Acc: 94.38%, Val. F1: 0.944, Current learning rate: 0.003887\n",
      "Epoch: 18, Train Loss: 0.051, Train Acc: 98.52%, Train F1: 0.985, Val. Loss: 0.171, Val. Acc: 95.24%, Val. F1: 0.952, Current learning rate: 0.003035\n",
      "Epoch: 19, Train Loss: 0.050, Train Acc: 98.81%, Train F1: 0.988, Val. Loss: 0.170, Val. Acc: 94.96%, Val. F1: 0.950, Current learning rate: 0.002246\n",
      "Epoch: 20, Train Loss: 0.054, Train Acc: 98.56%, Train F1: 0.986, Val. Loss: 0.168, Val. Acc: 94.81%, Val. F1: 0.948, Current learning rate: 0.001545\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 25\n",
    "patience = 3  # Количество эпох без улучшения перед остановкой\n",
    "best_val_loss = float('inf')\n",
    "early_stop_counter = 0  # Счётчик эпох без улучшений\n",
    "\n",
    "# Планировщик для изменения постоянной обучения\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=0.01, steps_per_epoch=len(train_loader), epochs=n_epochs\n",
    ")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Обучение модели\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    train_preds = []\n",
    "    train_targets = []\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(X_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        correct_train += (predictions.argmax(1) == y_batch).sum().item()\n",
    "        total_train += y_batch.size(0)\n",
    "\n",
    "        # Для F1-метрики\n",
    "        train_preds.extend(predictions.argmax(1).cpu().numpy())\n",
    "        train_targets.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    train_accuracy = correct_train / total_train\n",
    "    train_f1 = f1_score(train_targets, train_preds, average='macro')\n",
    "\n",
    "    # Оценка на валидационном наборе\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    val_preds = []\n",
    "    val_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            val_loss += loss.item()\n",
    "            correct_val += (predictions.argmax(1) == y_batch).sum().item()\n",
    "            total_val += y_batch.size(0)\n",
    "\n",
    "            # Для F1-метрики\n",
    "            val_preds.extend(predictions.argmax(1).cpu().numpy())\n",
    "            val_targets.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    val_accuracy = correct_val / total_val\n",
    "    val_loss /= len(val_loader)\n",
    "    val_f1 = f1_score(val_targets, val_preds, average='macro')\n",
    "\n",
    "    # Вывод метрик за эпоху\n",
    "    current_lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch: {epoch + 1:02}, \"\n",
    "          f\"Train Loss: {train_loss / len(train_loader):.3f}, \"\n",
    "          f\"Train Acc: {train_accuracy * 100:.2f}%, \"\n",
    "          f\"Train F1: {train_f1:.3f}, \"\n",
    "          f\"Val. Loss: {val_loss:.3f}, \"\n",
    "          f\"Val. Acc: {val_accuracy * 100:.2f}%, \"\n",
    "          f\"Val. F1: {val_f1:.3f}, \"\n",
    "          f\"Current learning rate: {current_lr:.6f}\")\n",
    "\n",
    "    # Ранняя остановка: проверка улучшения валидационного лосса\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss  # Сохраняем лучшее значение функции потерь\n",
    "        early_stop_counter = 0  # Сбрасываем счётчик\n",
    "        # Сохраняем лучшую модель\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "    else:\n",
    "        early_stop_counter += 1  # Увеличиваем счётчик эпох без улучшений\n",
    "\n",
    "    # Прекращение обучения, если модель на плато\n",
    "    if early_stop_counter >= patience:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FEND2cAY5Cru",
    "outputId": "e74cc136-144b-475c-c47a-be929b037cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.091, Test Accuracy: 91.92%, Test F1: 0.919\n"
     ]
    }
   ],
   "source": [
    "# Финальная проверка на тестовых данных\n",
    "model.eval()  # Перевод модели в режим оценки\n",
    "test_loss = 0\n",
    "correct_test = 0\n",
    "total_test = 0\n",
    "\n",
    "test_preds = []\n",
    "test_targets = []\n",
    "\n",
    "with torch.no_grad():  # Отключение вычисления градиентов\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        predictions = model(X_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        test_loss += loss.item()\n",
    "        correct_test += (predictions.argmax(1) == y_batch).sum().item()\n",
    "        total_test += y_batch.size(0)\n",
    "\n",
    "        test_preds.extend(predictions.argmax(1).cpu().numpy())\n",
    "        test_targets.extend(y_batch.cpu().numpy())\n",
    "\n",
    "\n",
    "test_accuracy = correct_test / total_test\n",
    "test_loss /= len(test_loader)\n",
    "test_f1 = f1_score(test_targets, test_preds, average='macro')\n",
    "\n",
    "test_accuracy = correct_test / total_test\n",
    "\n",
    "print(f\"Test Loss: {test_loss / len(test_loader):.3f}, \"\n",
    "      f\"Test Accuracy: {test_accuracy * 100:.2f}%, \"\n",
    "      f\"Test F1: {test_f1:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
