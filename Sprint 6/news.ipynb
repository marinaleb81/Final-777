{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2974\n"
     ]
    }
   ],
   "source": [
    "# Выбор новостных категорий из датасета\n",
    "categories = ['sci.med','sci.electronics', 'sci.space', 'rec.sport.baseball', 'soc.religion.christian']\n",
    "\n",
    "news_data = fetch_20newsgroups(subset='train', categories=categories)\n",
    "test_news_data = fetch_20newsgroups(subset='test', categories=categories)\n",
    "# Количество документов в наборе данных\n",
    "print(len(news_data.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: wright@duca.hi.com (David Wright)\n",
      "Subject: Re: NATURAL ANTI-cancer/AIDS Remedies\n",
      "Organization: Hitachi Computer Products, OSSD division\n",
      "Lines: 16\n",
      "NNTP-Posting-Host: duca.hi.com\n",
      "\n",
      "In article <19604@pitt.UUCP> geb@cs.pitt.edu (Gordon Banks) writes:\n",
      "|In article <1993Apr6.165840.5703@cnsvax.uwec.edu> mcelwre@cnsvax.uwec.edu writes:\n",
      "|>     The biggest reason why the cost of medical care is so EXTREMELY high and\n",
      "|>increasing is that NATURAL methods of treatment and even diagnosis are still\n",
      "|>being SYSTEMATICALLY IGNORED and SUPPRESSED by the MONEY-GRUBBING and POWER-\n",
      "|>MONGERING \"medical\" establishment.\n",
      "\n",
      "|That's not the half of it.  Did you realize that all medical doctors have\n",
      "|now been replaced by aliens?\n",
      "\n",
      "Yup.  By the way, what planet are you from, and once you got here, did\n",
      "you encounter those prejudices against foreign medical graduates?\n",
      "\n",
      "  -- David Wright, Hitachi Computer Products (America), Inc.  Waltham, MA\n",
      "     wright@hicomb.hi.com  ::  These are my opinions, not necessarily \n",
      "     Hitachi's, though they are the opinions of all right-thinking people\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Пример документа\n",
    "print(news_data.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: pyron@skndiv.dseg.ti.com (Dillon Pyron)\n",
      "Subject: Re: Shuttle oxygen (was Budget Astronaut)\n",
      "Lines: 24\n",
      "Nntp-Posting-Host: skndiv.dseg.ti.com\n",
      "Reply-To: pyron@skndiv.dseg.ti.com\n",
      "Organization: TI/DSE\n",
      "--------------------------------------------------\n",
      "From: tbrent@ecn.purdue.edu (Timothy J Brent)\n",
      "Subject: Am I going to Hell?\n",
      "Organization: Purdue University Engineering Computer Network\n",
      "Lines: 12\n",
      "\n",
      "I have stated before that I do not consider myself an\n",
      "--------------------------------------------------\n",
      "From: henry@zoo.toronto.edu (Henry Spencer)\n",
      "Subject: Re: Public-domain circuits in commercial applications\n",
      "Organization: U of Toronto Zoology\n",
      "Lines: 31\n",
      "\n",
      "In article <1993Apr13.164924.2606@wuecl.wustl.e\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Проверим наличие и структуру метаданных в документах\n",
    "\n",
    "import random\n",
    "\n",
    "num_rand_indices = 3\n",
    "max_index = len(news_data.data)\n",
    "document_num = [random.randint(0, max_index - 1) for _ in range(num_rand_indices)]\n",
    "\n",
    "def metadata_demo(data):\n",
    "    for i in document_num:\n",
    "        print(data.data[i][:200])\n",
    "        print(\"-\" * 50)\n",
    "metadata_demo(news_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skndivdsegticom\n",
      "ReplyTo \n",
      "In article 1qn044  Pat writes\n",
      "DFW was designed with the STS in mind which really mean very little  Much of\n",
      "their early PR material had scenes with a shuttle landing and two or\n",
      "--------------------------------------------------\n",
      "I have stated before that I do not consider myself an atheist but \n",
      "definitely do not believe in the christian god  The recent discussion\n",
      "about atheists and hell combined with a post to another group t\n",
      "--------------------------------------------------\n",
      "In article   David Prutchi writes\n",
      "There are two issues here  copyright and patent\n",
      "The magazine articles contents are copyrighted and may not be reproduced\n",
      "translated etc without the copyright holders \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Удаление метаданных, символов и пунктуации\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Удаление метаданных (начинающихся с ключевых слов)\n",
    "    text = re.sub(r'^(From|Subject|Organization|Lines):.*$', '', text, flags=re.MULTILINE)\n",
    "    # Удаление цитат (строки, начинающиеся с \">\")\n",
    "    text = re.sub(r'^>.*$', '', text, flags=re.MULTILINE)\n",
    "    # Удаление пустых строк\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    # Удаление адресов электронной почты\n",
    "    text = re.sub(r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})|()', '', text)\n",
    "    # Удаление символов и пунктуации\n",
    "    text = re.sub(r'[!\"#$%&\\'()*+,./:;<=>?@\\[\\]^_`{|}~«»—*\\-\\—]', '', text)\n",
    "    text = re.sub(r'\\b\\w*postinghost\\w*\\b', '', text, flags=re.IGNORECASE)\n",
    "    return text.strip()\n",
    "\n",
    "news_data.data = [clean_text(document) for document in news_data.data]\n",
    "test_news_data.data = [clean_text(document) for document in test_news_data.data]\n",
    "\n",
    "metadata_demo(news_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\qqqq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\qqqq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\qqqq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\qqqq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Токенизация по словам\n",
    "tokenized_news_data = [word_tokenize(document.lower()) for document in news_data.data]\n",
    "tokenized_test_news_data = [word_tokenize(document.lower()) for document in test_news_data.data]\n",
    "# Удаление стоп-слов\n",
    "stop_words = sorted(stopwords.words('english'))\n",
    "nostopword_tokenized_news_data = [\n",
    "    [w for w in document if w.strip() and w not in stop_words]\n",
    "    for document in tokenized_news_data\n",
    "]\n",
    "nostopword_tokenized_test_news_data = [\n",
    "    [w for w in document if w.strip() and w not in stop_words]\n",
    "    for document in tokenized_test_news_data\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ducahicom', 'article', 'gordon', 'bank', 'writes', 'article', 'writes', 'biggest', 'reason', 'cost', 'medical', 'care', 'extremely', 'high', 'increasing', 'natural', 'method', 'treatment', 'even', 'diagnosis', 'still', 'systematically', 'ignored', 'suppressed', 'moneygrubbing', 'power', 'mongering', 'medical', 'establishment', 'thats', 'half', 'realize', 'medical', 'doctor', 'replaced', 'alien', 'yup', 'way', 'planet', 'got', 'encounter', 'prejudice', 'foreign', 'medical', 'graduate', 'david', 'wright', 'hitachi', 'computer', 'product', 'america', 'inc', 'waltham', 'opinion', 'necessarily', 'hitachis', 'though', 'opinion', 'rightthinking', 'people']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk_lemmatizer = WordNetLemmatizer()\n",
    "# Лемматизация обработанного двумерного массива слов\n",
    "lemmatized_tokens = [\n",
    "    [nltk_lemmatizer.lemmatize(w) for w in document] \n",
    "    for document in nostopword_tokenized_news_data\n",
    "]\n",
    "lemmatized_test_tokens = [\n",
    "    [nltk_lemmatizer.lemmatize(w) for w in document] \n",
    "    for document in nostopword_tokenized_test_news_data\n",
    "]\n",
    "\n",
    "# Преобразование вложенных массивов в строки\n",
    "lemmatized_documents = [' '.join(tokens) for tokens in lemmatized_tokens]\n",
    "lemmatized_test_documents = [' '.join(tokens) for tokens in lemmatized_test_tokens]\n",
    "\n",
    "print(lemmatized_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_LR = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(stop_words='english')),\n",
    "    ('logistic regression', LogisticRegressionCV(cv=3, n_jobs=-1))\n",
    "])\n",
    "model2_LR = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(stop_words='english', ngram_range=(1, 2))),\n",
    "    ('logistic regression', LogisticRegressionCV(cv=3, n_jobs=-1))\n",
    "])\n",
    "model3_LR = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(stop_words='english', ngram_range=(1, 2))),\n",
    "    ('logistic regression', LogisticRegressionCV(cv=5, n_jobs=-1))\n",
    "])\n",
    "model4_RFC = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(stop_words='english', ngram_range=(1, 2))),\n",
    "    ('random_forest', RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train for model1_LR: 0.9997\n",
      "Accuracy on test for model1_LR: 0.9014\n",
      "Train time for model1_LR: 5.97510552406311\n",
      "Accuracy on train for model2_LR: 0.9997\n",
      "Accuracy on test for model2_LR: 0.9130\n",
      "Train time for model2_LR: 23.107374906539917\n",
      "Accuracy on train for model3_LR: 0.9997\n",
      "Accuracy on test for model3_LR: 0.9130\n",
      "Train time for model3_LR: 40.11469054222107\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model1_LR.fit(lemmatized_documents, news_data.target)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "train_accuracy = accuracy_score(news_data.target, model1_LR.predict(lemmatized_documents))\n",
    "test_accuracy = accuracy_score(test_news_data.target, model1_LR.predict(lemmatized_test_documents))\n",
    "print(f'Accuracy on train for model1_LR: {train_accuracy:.4f}\\nAccuracy on test for model1_LR: {test_accuracy:.4f}')\n",
    "print(f'Train time for model1_LR: {train_time}')\n",
    "\n",
    "start_time = time.time()\n",
    "model2_LR.fit(lemmatized_documents, news_data.target)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "train_accuracy = accuracy_score(news_data.target, model2_LR.predict(lemmatized_documents))\n",
    "test_accuracy = accuracy_score(test_news_data.target, model2_LR.predict(lemmatized_test_documents))\n",
    "print(f'Accuracy on train for model2_LR: {train_accuracy:.4f}\\nAccuracy on test for model2_LR: {test_accuracy:.4f}')\n",
    "print(f'Train time for model2_LR: {train_time}')\n",
    "\n",
    "start_time = time.time()\n",
    "model3_LR.fit(lemmatized_documents, news_data.target)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "train_accuracy = accuracy_score(news_data.target, model3_LR.predict(lemmatized_documents))\n",
    "test_accuracy = accuracy_score(test_news_data.target, model3_LR.predict(lemmatized_test_documents))\n",
    "print(f'Accuracy on train for model3_LR: {train_accuracy:.4f}\\nAccuracy on test for model3_LR: {test_accuracy:.4f}')\n",
    "print(f'Train time for model3_LR: {train_time}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train for model4_RFC: 0.9997\n",
      "Accuracy on test for model4_RFC: 0.8037\n",
      "Train time for model4_RFC: 5.444522142410278\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model4_RFC.fit(lemmatized_documents, news_data.target)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "train_accuracy = accuracy_score(news_data.target, model4_RFC.predict(lemmatized_documents))\n",
    "test_accuracy = accuracy_score(test_news_data.target, model4_RFC.predict(lemmatized_test_documents))\n",
    "print(f'Accuracy on train for model4_RFC: {train_accuracy:.4f}\\nAccuracy on test for model4_RFC: {test_accuracy:.4f}')\n",
    "print(f'Train time for model4_RFC: {train_time}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
