{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4952\n"
     ]
    }
   ],
   "source": [
    "# Выбор новостных категорий из датасета\n",
    "categories = ['sci.med','sci.electronics', 'sci.space', 'rec.sport.baseball', 'soc.religion.christian']\n",
    "\n",
    "news_data = fetch_20newsgroups(subset='all', categories=categories)\n",
    "\n",
    "# Создание обучающей и тестовой выборок\n",
    "X, y = news_data.data, news_data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.45, random_state=42)\n",
    "\n",
    "# Количество документов в наборе данных\n",
    "print(len(news_data.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Organization: University of Illinois at Chicago, academic Computer Center\n",
      "From: <U49839@uicvm.uic.edu>\n",
      "Subject: Re: Harry Caray\n",
      "Distribution: na\n",
      "Lines: 17\n",
      "\n",
      "\n",
      "last night bill veeck cam to me in my dreams and this is what he said:\n",
      "\n",
      "cubs suck cubs suck cubs suck cubs suck cubs suck cubs suck cubs suck\n",
      "cubs suck cubs suck cubs suck cubs suck cubs suck cubs suck cubs scuk\n",
      "cubs suck cubs suck cubs suck cubs cuck cubs suck cubs suck cubs suck\n",
      "cubs suck cubs suck cubs suck cubs suck cubs suck cubs suck cubs suck\n",
      "cubs suck cubs suck cubs suck cubs suck cubs suck cubs suck cubs suck\n",
      "\n",
      "oh yeah, he aqlso added that harry is a drunken idiot who shoulda\n",
      "stayed in st louis where his heart is, but also added that fair weathered\n",
      "fans all like to be together.  i guess this is the reason harry is now\n",
      "a cub fan, bud man.  note he never really left st, louis.\n",
      "\n",
      "jim walker\n",
      "\n",
      "go sox, cubs suck!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Пример документа\n",
    "print(news_data.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: sbishop@desire.wright.edu\n",
      "Subject: Re: Hismanal, et. al.--side effects\n",
      "Organization:  Wright State University \n",
      "Lines: 22\n",
      "\n",
      "In article <1993Apr21.024103.29880@spdcc.com>, dyer@spdcc.com (Steve Dye\n",
      "--------------------------------------------------\n",
      "From: hudson@athena.cs.uga.edu (Paul Hudson Jr)\n",
      "Subject: Re: Hell_2:  Black Sabbath\n",
      "Organization: University of Georgia, Athens\n",
      "Lines: 8\n",
      "\n",
      "In article <Apr.22.00.57.03.1993.2118@geneva.rutgers.edu> jprz\n",
      "--------------------------------------------------\n",
      "From: geb@cs.pitt.edu (Gordon Banks)\n",
      "Subject: Re: CAN'T BREATHE\n",
      "Article-I.D.: pitt.19438\n",
      "Reply-To: geb@cs.pitt.edu (Gordon Banks)\n",
      "Organization: Univ. of Pittsburgh Computer Science\n",
      "Lines: 33\n",
      "\n",
      "In artic\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Проверим наличие и структуру метаданных в документах\n",
    "\n",
    "import random\n",
    "\n",
    "def metadata_demo(data, num_indices = 3):\n",
    "    max_index = len(data)\n",
    "    document_num = [random.randint(0, max_index - 1) for _ in range(num_indices)]\n",
    "    for i in document_num:\n",
    "        print(data[i][:200])\n",
    "        print(\"-\" * 50)\n",
    "metadata_demo(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords WHY\n",
      "In article   William Pollak writes\n",
      "Deletions\n",
      "Geez Dal must have slipped something into Teds drink sometime  Comparing\n",
      "Prince to Pagnozzi offensively is laughable  Prince has never hit wel\n",
      "--------------------------------------------------\n",
      "NntpPostingHost aisunaiugaedu\n",
      "In article   Peter Tryndoch writes\n",
      "Up to  microamperes     on hook\n",
      "Over something like  mA   off hook\n",
      "In between  defective line and the phone company comes looking\n",
      "     \n",
      "--------------------------------------------------\n",
      "Keywords hearing loss vitamin A\n",
      "ArticleID bananaApr\n",
      "Distribution sci\n",
      "i heard a news report indicating research showing improved         \n",
      "hearing in people taking vitamin A the research showed that new\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Удаление метаданных, символов и пунктуации\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Удаление метаданных (начинающихся с ключевых слов)\n",
    "    text = re.sub(r'^(From|Subject|Organization|Lines):.*$', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\b\\w*postinghost\\w*\\b', '', text, flags=re.IGNORECASE)\n",
    "    # Удаление цитат (строки, начинающиеся с \">\")\n",
    "    text = re.sub(r'^>.*$', '', text, flags=re.MULTILINE)\n",
    "    # Удаление пустых строк\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    # Удаление адресов электронной почты\n",
    "    text = re.sub(r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})|()', '', text)\n",
    "    # Удаление символов и пунктуации\n",
    "    text = re.sub(r'[!\"#$%&\\'()*+,./:;<=>?@\\[\\]^_`{|}~«»—*\\-\\—]', '', text)\n",
    "    # Удаление чисел\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "X_train = [clean_text(document) for document in X_train]\n",
    "X_test = [clean_text(document) for document in X_test]\n",
    "\n",
    "metadata_demo(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\qqqq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\qqqq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\qqqq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\qqqq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Токенизация по словам\n",
    "tokenized_news_data = [word_tokenize(document.lower()) for document in X_train]\n",
    "tokenized_test_news_data = [word_tokenize(document.lower()) for document in X_test]\n",
    "# Удаление стоп-слов\n",
    "stop_words = sorted(stopwords.words('english'))\n",
    "nostopword_tokenized_news_data = [\n",
    "    [w for w in document if w.strip() and w not in stop_words]\n",
    "    for document in tokenized_news_data\n",
    "]\n",
    "nostopword_tokenized_test_news_data = [\n",
    "    [w for w in document if w.strip() and w not in stop_words]\n",
    "    for document in tokenized_test_news_data\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['distribution', 'world', 'nntppostinghost', 'dolphinzoocsyaleedu', 'recently', 'ive', 'come', 'upon', 'body', 'literature', 'promotes', 'colon', 'cleansing', 'vital', 'aid', 'preventive', 'medicine', 'nutrition', 'particular', 'dr', 'bernard', 'jenssen', 'book', 'colon', 'cleansing', 'health', 'longevity', 'title', 'actually', 'escape', 'similar', 'claim', 'regular', 'selfadministered', 'colonic', 'along', 'certain', 'orally', 'ingested', 'debrisloosening', 'agent', 'boost', 'immune', 'system', 'significant', 'degree', 'also', 'plug', 'unique', 'appliance', 'called', 'colema', 'board', 'facilitates', 'selfadministration', 'colonic', 'sell', 'californiabased', 'company', 'also', 'plug', 'vitratox', 'product', 'chemical', 'agent', 'choice', 'include', 'volcanic', 'ash', 'supposedly', 'electrical', 'charge', 'psyllium', 'powder', 'bulkiness', 'anyone', 'know', 'anything', 'colon', 'cleansing', 'theory', 'particular', 'colema', 'board', 'related', 'product', 'id', 'interested', 'hear', 'research', 'personal', 'experience', 'article', 'crossposted', 'altmagick', 'issue', 'touch', 'upon', 'fasting', 'cleansing', 'ritual', 'system', 'purification', 'eli', '\\\\', 'elisha', 'wiesel', 'davenport', 'college', 'yale', 'university', 'school', 'home', '\\\\']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk_lemmatizer = WordNetLemmatizer()\n",
    "# Лемматизация обработанного двумерного массива слов\n",
    "lemmatized_tokens = [\n",
    "    [nltk_lemmatizer.lemmatize(w) for w in document] \n",
    "    for document in nostopword_tokenized_news_data\n",
    "]\n",
    "lemmatized_test_tokens = [\n",
    "    [nltk_lemmatizer.lemmatize(w) for w in document] \n",
    "    for document in nostopword_tokenized_test_news_data\n",
    "]\n",
    "\n",
    "# Преобразование вложенных массивов в строки\n",
    "lemmatized_documents = [' '.join(tokens) for tokens in lemmatized_tokens]\n",
    "lemmatized_test_documents = [' '.join(tokens) for tokens in lemmatized_test_tokens]\n",
    "\n",
    "print(lemmatized_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация моделей для классификации\n",
    "\n",
    "# Векторизация по униграммам\n",
    "model1_LR = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(stop_words='english')),\n",
    "    ('logistic regression', LogisticRegressionCV(cv=3, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Векторизация по униграммам и биграммам \n",
    "model2_LR = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(stop_words='english', ngram_range=(1, 2))),\n",
    "    ('logistic regression', LogisticRegressionCV(cv=3, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Векторизация по униграммам и биграммам, параметр кросс-валидации увеличен до cv=5\n",
    "model3_LR = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(stop_words='english', ngram_range=(1, 2))),\n",
    "    ('logistic regression', LogisticRegressionCV(cv=5, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Векторизация по униграммам и триграммам\n",
    "model4_LR = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(stop_words='english', ngram_range=(2, 3))),\n",
    "    ('logistic regression', LogisticRegressionCV(cv=3, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Метод случайного леса\n",
    "model5_RFC = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(stop_words='english', ngram_range=(1, 2))),\n",
    "    ('random_forest', RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train for model1_LR: 1.0000\n",
      "Accuracy on test for model1_LR: 0.9480\n",
      "Train time for model1_LR: 4.732477903366089\n",
      "Accuracy on train for model2_LR: 1.0000\n",
      "Accuracy on test for model2_LR: 0.9556\n",
      "Train time for model2_LR: 20.87546396255493\n",
      "Accuracy on train for model3_LR: 1.0000\n",
      "Accuracy on test for model3_LR: 0.9547\n",
      "Train time for model3_LR: 33.637930393218994\n",
      "Accuracy on train for model4_LR: 1.0000\n",
      "Accuracy on test for model4_LR: 0.8618\n",
      "Train time for model4_LR: 30.345017671585083\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model1_LR.fit(lemmatized_documents, y_train)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, model1_LR.predict(lemmatized_documents))\n",
    "test_accuracy = accuracy_score(y_test, model1_LR.predict(lemmatized_test_documents))\n",
    "print(f'Accuracy on train for model1_LR: {train_accuracy:.4f}\\nAccuracy on test for model1_LR: {test_accuracy:.4f}')\n",
    "print(f'Train time for model1_LR: {train_time}')\n",
    "\n",
    "start_time = time.time()\n",
    "model2_LR.fit(lemmatized_documents, y_train)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, model2_LR.predict(lemmatized_documents))\n",
    "test_accuracy = accuracy_score(y_test, model2_LR.predict(lemmatized_test_documents))\n",
    "print(f'Accuracy on train for model2_LR: {train_accuracy:.4f}\\nAccuracy on test for model2_LR: {test_accuracy:.4f}')\n",
    "print(f'Train time for model2_LR: {train_time}')\n",
    "\n",
    "start_time = time.time()\n",
    "model3_LR.fit(lemmatized_documents, y_train)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, model3_LR.predict(lemmatized_documents))\n",
    "test_accuracy = accuracy_score(y_test, model3_LR.predict(lemmatized_test_documents))\n",
    "print(f'Accuracy on train for model3_LR: {train_accuracy:.4f}\\nAccuracy on test for model3_LR: {test_accuracy:.4f}')\n",
    "print(f'Train time for model3_LR: {train_time}')\n",
    "\n",
    "start_time = time.time()\n",
    "model4_LR.fit(lemmatized_documents, y_train)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, model4_LR.predict(lemmatized_documents))\n",
    "test_accuracy = accuracy_score(y_test, model4_LR.predict(lemmatized_test_documents))\n",
    "print(f'Accuracy on train for model4_LR: {train_accuracy:.4f}\\nAccuracy on test for model4_LR: {test_accuracy:.4f}')\n",
    "print(f'Train time for model4_LR: {train_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train for model4_RFC: 1.0000\n",
      "Accuracy on test for model4_RFC: 0.8816\n",
      "Train time for model4_RFC: 10.096070051193237\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model5_RFC.fit(lemmatized_documents, y_train)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, model5_RFC.predict(lemmatized_documents))\n",
    "test_accuracy = accuracy_score(y_test, model5_RFC.predict(lemmatized_test_documents))\n",
    "print(f'Accuracy on train for model4_RFC: {train_accuracy:.4f}\\nAccuracy on test for model4_RFC: {test_accuracy:.4f}')\n",
    "print(f'Train time for model4_RFC: {train_time}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
