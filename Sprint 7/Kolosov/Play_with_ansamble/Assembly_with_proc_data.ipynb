{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f495e5b-a73b-4acb-98be-ac62fc88abeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.5.0-py3-none-macosx_10_15_x86_64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Applications/Anaconda/anaconda3/lib/python3.12/site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in /Applications/Anaconda/anaconda3/lib/python3.12/site-packages (from lightgbm) (1.13.1)\n",
      "Downloading lightgbm-4.5.0-py3-none-macosx_10_15_x86_64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7df73b4a-96ae-448e-8633-6aeadbfb026f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.3-py3-none-macosx_10_15_x86_64.macosx_11_0_x86_64.macosx_12_0_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in /Applications/Anaconda/anaconda3/lib/python3.12/site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in /Applications/Anaconda/anaconda3/lib/python3.12/site-packages (from xgboost) (1.13.1)\n",
      "Downloading xgboost-2.1.3-py3-none-macosx_10_15_x86_64.macosx_11_0_x86_64.macosx_12_0_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dde4f35-8bd3-4ba9-b73f-d9fdacd98ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import VotingClassifier, BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier, StackingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c37e9d27-2f25-45d7-a1b9-50e66c8c8a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "До SMOTE: Counter({0: 134238, 1: 2122})\n",
      "После SMOTE: Counter({0: 134238, 1: 134238})\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data_train_proc_FINAL.csv', sep=';') \n",
    "\n",
    "X = data.drop('Machine failure', axis=1)  \n",
    "y = data['Machine failure']\n",
    "\n",
    "print(\"До SMOTE:\", Counter(y))\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "print(\"После SMOTE:\", Counter(y_resampled))\n",
    "\n",
    "X_resampled_df = pd.DataFrame(X_resampled)\n",
    "y_resampled_df = pd.DataFrame(y_resampled, columns=['Machine failure'])\n",
    "\n",
    "balanced_data = pd.concat([X_resampled_df, y_resampled_df], axis=1)\n",
    "balanced_data.to_csv(\"balanced_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dcce439-ab60-4613-9fdf-d3eadf47246a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.width', 1900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d052c6ed-551e-4162-b066-410486fec2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sum_Parameter  Rotational speed [rpm]  Torque [Nm]  Tool wear [min]  Air temperature [K]  Process temperature [K]  Machine failure\n",
      "0              0                    1596         36.1              140                300.6                    309.6                0\n",
      "1              0                    1759         29.1              200                302.6                    312.1                0\n",
      "2              0                    1805         26.5               25                299.3                    308.5                0\n",
      "3              0                    1524         44.3              197                301.0                    310.9                0\n",
      "4              0                    1641         35.4               34                298.0                    309.0                0\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('balanced_dataset.csv') \n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7e47898-45a9-43f1-a861-377532ecd552",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('Machine failure', axis=1)  \n",
    "y = data['Machine failure']\n",
    "\n",
    "X.columns = X.columns.astype(str)  \n",
    "X.columns = X.columns.str.replace(r'[\\[\\]<]', '', regex=True)\n",
    "X.columns = X.columns.str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9914335-f717-4c33-b38b-b8d83fd821b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "170bb321-f105-4e19-a6cd-b95c2559ff19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверим сразу несколько, чтобы удобно было сравнить\n",
    "# Список моделей\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric='logloss', random_state=42),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42),\n",
    "    \"SVM\": SVC(random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4506bf1a-76ab-4be1-b171-1e359ce12256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 107390, number of negative: 107390\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000603 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1279\n",
      "[LightGBM] [Info] Number of data points in the train set: 214780, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    }
   ],
   "source": [
    "# Сравнение моделей\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)  \n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": report['weighted avg']['precision'],\n",
    "        \"Recall\": report['weighted avg']['recall'],\n",
    "        \"F1-Score\": report['weighted avg']['f1-score']\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47410ba3-29cf-4ae7-b15f-41e97a29ce2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавление ансамбля моделей\n",
    "model1 = LogisticRegression(random_state=42)\n",
    "model2 = RandomForestClassifier(random_state=42)\n",
    "model3 = XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "\n",
    "ensemble = VotingClassifier(estimators=[\n",
    "    ('lr', model1), ('rf', model2), ('xgb', model3)], voting='soft')\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Прогнозирование ансамбля\n",
    "y_pred_ensemble = ensemble.predict(X_test)\n",
    "ensemble_accuracy = accuracy_score(y_test, y_pred_ensemble)\n",
    "ensemble_report = classification_report(y_test, y_pred_ensemble, output_dict=True)\n",
    "results.append({\n",
    "    \"Model\": \"Voting Ensemble\",\n",
    "    \"Accuracy\": ensemble_accuracy,\n",
    "    \"Precision\": ensemble_report['weighted avg']['precision'],\n",
    "    \"Recall\": ensemble_report['weighted avg']['recall'],\n",
    "    \"F1-Score\": ensemble_report['weighted avg']['f1-score']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4e698318-c79c-4bca-a068-5766b249081a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_model = BaggingClassifier(estimator=RandomForestClassifier(random_state=42), n_estimators=50, random_state=42)\n",
    "bagging_model.fit(X_train, y_train)\n",
    "y_pred_bagging = bagging_model.predict(X_test)\n",
    "bagging_accuracy = accuracy_score(y_test, y_pred_bagging)\n",
    "bagging_report = classification_report(y_test, y_pred_bagging, output_dict=True)\n",
    "results.append({\n",
    "    \"Model\": \"Bagging\",\n",
    "    \"Accuracy\": bagging_accuracy,\n",
    "    \"Precision\": bagging_report['weighted avg']['precision'],\n",
    "    \"Recall\": bagging_report['weighted avg']['recall'],\n",
    "    \"F1-Score\": bagging_report['weighted avg']['f1-score']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "42490453-0a14-4e2a-aaf2-393742b314b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "gb_accuracy = accuracy_score(y_test, y_pred_gb)\n",
    "gb_report = classification_report(y_test, y_pred_gb, output_dict=True)\n",
    "results.append({\n",
    "    \"Model\": \"Gradient Boosting\",\n",
    "    \"Accuracy\": gb_accuracy,\n",
    "    \"Precision\": gb_report['weighted avg']['precision'],\n",
    "    \"Recall\": gb_report['weighted avg']['recall'],\n",
    "    \"F1-Score\": gb_report['weighted avg']['f1-score']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2dfca34f-3dad-4d07-81b9-557e80e8deb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 107390, number of negative: 107390\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000886 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1279\n",
      "[LightGBM] [Info] Number of data points in the train set: 214780, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 85912, number of negative: 85912\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000717 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1279\n",
      "[LightGBM] [Info] Number of data points in the train set: 171824, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 85912, number of negative: 85912\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000502 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1277\n",
      "[LightGBM] [Info] Number of data points in the train set: 171824, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 85912, number of negative: 85912\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000485 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1279\n",
      "[LightGBM] [Info] Number of data points in the train set: 171824, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 85912, number of negative: 85912\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000467 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1279\n",
      "[LightGBM] [Info] Number of data points in the train set: 171824, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 85912, number of negative: 85912\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000519 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1278\n",
      "[LightGBM] [Info] Number of data points in the train set: 171824, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    }
   ],
   "source": [
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', RandomForestClassifier(random_state=42)),\n",
    "        ('xgb', XGBClassifier(eval_metric='logloss', random_state=42)),\n",
    "        ('lgbm', LGBMClassifier(random_state=42))\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(random_state=42)\n",
    ")\n",
    "stacking_model.fit(X_train, y_train)\n",
    "y_pred_stacking = stacking_model.predict(X_test)\n",
    "stacking_accuracy = accuracy_score(y_test, y_pred_stacking)\n",
    "stacking_report = classification_report(y_test, y_pred_stacking, output_dict=True)\n",
    "results.append({\n",
    "    \"Model\": \"Stacking\",\n",
    "    \"Accuracy\": stacking_accuracy,\n",
    "    \"Precision\": stacking_report['weighted avg']['precision'],\n",
    "    \"Recall\": stacking_report['weighted avg']['recall'],\n",
    "    \"F1-Score\": stacking_report['weighted avg']['f1-score']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d9a5e99d-f725-4846-b2e2-614ebd0a5d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/Anaconda/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "adaboost_model = AdaBoostClassifier(estimator=RandomForestClassifier(random_state=42), n_estimators=50, random_state=42)\n",
    "adaboost_model.fit(X_train, y_train)\n",
    "y_pred_adaboost = adaboost_model.predict(X_test)\n",
    "adaboost_accuracy = accuracy_score(y_test, y_pred_adaboost)\n",
    "adaboost_report = classification_report(y_test, y_pred_adaboost, output_dict=True)\n",
    "results.append({\n",
    "    \"Model\": \"AdaBoost\",\n",
    "    \"Accuracy\": adaboost_accuracy,\n",
    "    \"Precision\": adaboost_report['weighted avg']['precision'],\n",
    "    \"Recall\": adaboost_report['weighted avg']['recall'],\n",
    "    \"F1-Score\": adaboost_report['weighted avg']['f1-score']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e099972d-f8a6-4ccf-a5ee-de6e02c080a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model  Accuracy  Precision    Recall  F1-Score\n",
      "0        Random Forest  0.987224   0.987229  0.987224  0.987224\n",
      "1              XGBoost  0.978732   0.978909  0.978732  0.978730\n",
      "2             LightGBM  0.981060   0.981204  0.981060  0.981059\n",
      "3  Logistic Regression  0.856172   0.865172  0.856172  0.855280\n",
      "4                  SVM  0.892338   0.897123  0.892338  0.892013\n",
      "5      Voting Ensemble  0.979105   0.979279  0.979105  0.979103\n",
      "6              Bagging  0.983779   0.983781  0.983779  0.983779\n",
      "7    Gradient Boosting  0.912917   0.914811  0.912917  0.912818\n",
      "8             Stacking  0.990986   0.990987  0.990986  0.990986\n",
      "9             AdaBoost  0.988007   0.988010  0.988007  0.988007\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d2c0b6b7-a3ad-4cad-8d7c-ddc0087e5ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Так же наилучший результат показал Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2788b95a-dfd6-4163-88bc-ca06c721e054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
