{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import VotingClassifier, BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier, StackingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "До SMOTE: Counter({0: 134281, 1: 2148})\n",
      "После SMOTE: Counter({0: 134281, 1: 134281})\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('processed_train.csv', sep=';') \n",
    "\n",
    "X = data.drop('Machine failure', axis=1)  \n",
    "y = data['Machine failure']\n",
    "\n",
    "print(\"До SMOTE:\", Counter(y))\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "print(\"После SMOTE:\", Counter(y_resampled))\n",
    "\n",
    "X_resampled_df = pd.DataFrame(X_resampled)\n",
    "y_resampled_df = pd.DataFrame(y_resampled, columns=['Machine failure'])\n",
    "\n",
    "balanced_data = pd.concat([X_resampled_df, y_resampled_df], axis=1)\n",
    "balanced_data.to_csv(\"balanced_dataset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.width', 1900)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  Air temperature [K]  Process temperature [K]  Rotational speed [rpm]  Torque [Nm]  Tool wear [min]  Tool Wear Failure [TWF]  Heat Dissipation Failure [HDF]  Power Failure [PWF]  Overstrain Failure [OSF]  Random Failure [RNF]  Type_H  Type_L  Type_M  Machine failure\n",
      "0   0                300.6                    309.6                    1596         36.1              140                        0                               0                    0                         0                     0   False    True   False                0\n",
      "1   1                302.6                    312.1                    1759         29.1              200                        0                               0                    0                         0                     0   False   False    True                0\n",
      "2   2                299.3                    308.5                    1802         26.5               25                        0                               0                    0                         0                     0   False    True   False                0\n",
      "3   3                301.0                    310.9                    1524         44.3              197                        0                               0                    0                         0                     0   False    True   False                0\n",
      "4   4                298.0                    309.0                    1641         35.4               34                        0                               0                    0                         0                     0   False   False    True                0\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('balanced_dataset.csv') \n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('Machine failure', axis=1)  \n",
    "y = data['Machine failure']\n",
    "\n",
    "X.columns = X.columns.astype(str)  \n",
    "X.columns = X.columns.str.replace(r'[\\[\\]<]', '', regex=True)\n",
    "X.columns = X.columns.str.replace(' ', '_')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверим сразу несколько, чтобы удобно было сравнить\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Список моделей\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric='logloss', random_state=42),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42),\n",
    "    \"SVM\": SVC(random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сравнение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 107425, number of negative: 107424\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004683 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1552\n",
      "[LightGBM] [Info] Number of data points in the train set: 214849, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500002 -> initscore=0.000009\n",
      "[LightGBM] [Info] Start training from score 0.000009\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)  \n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": report['weighted avg']['precision'],\n",
    "        \"Recall\": report['weighted avg']['recall'],\n",
    "        \"F1-Score\": report['weighted avg']['f1-score']\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Добавление ансамбля моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = LogisticRegression(random_state=42)\n",
    "model2 = RandomForestClassifier(random_state=42)\n",
    "model3 = XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "\n",
    "ensemble = VotingClassifier(estimators=[\n",
    "    ('lr', model1), ('rf', model2), ('xgb', model3)], voting='soft')\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Прогнозирование ансамбля\n",
    "y_pred_ensemble = ensemble.predict(X_test)\n",
    "ensemble_accuracy = accuracy_score(y_test, y_pred_ensemble)\n",
    "ensemble_report = classification_report(y_test, y_pred_ensemble, output_dict=True)\n",
    "results.append({\n",
    "    \"Model\": \"Voting Ensemble\",\n",
    "    \"Accuracy\": ensemble_accuracy,\n",
    "    \"Precision\": ensemble_report['weighted avg']['precision'],\n",
    "    \"Recall\": ensemble_report['weighted avg']['recall'],\n",
    "    \"F1-Score\": ensemble_report['weighted avg']['f1-score']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_model = BaggingClassifier(estimator=RandomForestClassifier(random_state=42), n_estimators=50, random_state=42)\n",
    "bagging_model.fit(X_train, y_train)\n",
    "y_pred_bagging = bagging_model.predict(X_test)\n",
    "bagging_accuracy = accuracy_score(y_test, y_pred_bagging)\n",
    "bagging_report = classification_report(y_test, y_pred_bagging, output_dict=True)\n",
    "results.append({\n",
    "    \"Model\": \"Bagging\",\n",
    "    \"Accuracy\": bagging_accuracy,\n",
    "    \"Precision\": bagging_report['weighted avg']['precision'],\n",
    "    \"Recall\": bagging_report['weighted avg']['recall'],\n",
    "    \"F1-Score\": bagging_report['weighted avg']['f1-score']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "gb_accuracy = accuracy_score(y_test, y_pred_gb)\n",
    "gb_report = classification_report(y_test, y_pred_gb, output_dict=True)\n",
    "results.append({\n",
    "    \"Model\": \"Gradient Boosting\",\n",
    "    \"Accuracy\": gb_accuracy,\n",
    "    \"Precision\": gb_report['weighted avg']['precision'],\n",
    "    \"Recall\": gb_report['weighted avg']['recall'],\n",
    "    \"F1-Score\": gb_report['weighted avg']['f1-score']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 107425, number of negative: 107424\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004672 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1552\n",
      "[LightGBM] [Info] Number of data points in the train set: 214849, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500002 -> initscore=0.000009\n",
      "[LightGBM] [Info] Start training from score 0.000009\n",
      "[LightGBM] [Info] Number of positive: 85940, number of negative: 85939\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003844 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1551\n",
      "[LightGBM] [Info] Number of data points in the train set: 171879, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500003 -> initscore=0.000012\n",
      "[LightGBM] [Info] Start training from score 0.000012\n",
      "[LightGBM] [Info] Number of positive: 85940, number of negative: 85939\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003581 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1551\n",
      "[LightGBM] [Info] Number of data points in the train set: 171879, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500003 -> initscore=0.000012\n",
      "[LightGBM] [Info] Start training from score 0.000012\n",
      "[LightGBM] [Info] Number of positive: 85940, number of negative: 85939\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003721 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1551\n",
      "[LightGBM] [Info] Number of data points in the train set: 171879, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500003 -> initscore=0.000012\n",
      "[LightGBM] [Info] Start training from score 0.000012\n",
      "[LightGBM] [Info] Number of positive: 85940, number of negative: 85939\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004357 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1551\n",
      "[LightGBM] [Info] Number of data points in the train set: 171879, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500003 -> initscore=0.000012\n",
      "[LightGBM] [Info] Start training from score 0.000012\n",
      "[LightGBM] [Info] Number of positive: 85940, number of negative: 85940\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003748 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1550\n",
      "[LightGBM] [Info] Number of data points in the train set: 171880, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    }
   ],
   "source": [
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', RandomForestClassifier(random_state=42)),\n",
    "        ('xgb', XGBClassifier(eval_metric='logloss', random_state=42)),\n",
    "        ('lgbm', LGBMClassifier(random_state=42))\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(random_state=42)\n",
    ")\n",
    "stacking_model.fit(X_train, y_train)\n",
    "y_pred_stacking = stacking_model.predict(X_test)\n",
    "stacking_accuracy = accuracy_score(y_test, y_pred_stacking)\n",
    "stacking_report = classification_report(y_test, y_pred_stacking, output_dict=True)\n",
    "results.append({\n",
    "    \"Model\": \"Stacking\",\n",
    "    \"Accuracy\": stacking_accuracy,\n",
    "    \"Precision\": stacking_report['weighted avg']['precision'],\n",
    "    \"Recall\": stacking_report['weighted avg']['recall'],\n",
    "    \"F1-Score\": stacking_report['weighted avg']['f1-score']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\0\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "adaboost_model = AdaBoostClassifier(estimator=RandomForestClassifier(random_state=42), n_estimators=50, random_state=42)\n",
    "adaboost_model.fit(X_train, y_train)\n",
    "y_pred_adaboost = adaboost_model.predict(X_test)\n",
    "adaboost_accuracy = accuracy_score(y_test, y_pred_adaboost)\n",
    "adaboost_report = classification_report(y_test, y_pred_adaboost, output_dict=True)\n",
    "results.append({\n",
    "    \"Model\": \"AdaBoost\",\n",
    "    \"Accuracy\": adaboost_accuracy,\n",
    "    \"Precision\": adaboost_report['weighted avg']['precision'],\n",
    "    \"Recall\": adaboost_report['weighted avg']['recall'],\n",
    "    \"F1-Score\": adaboost_report['weighted avg']['f1-score']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model  Accuracy  Precision    Recall  F1-Score\n",
      "0        Random Forest  0.970622   0.970630  0.970622  0.970621\n",
      "1              XGBoost  0.973712   0.973836  0.973712  0.973710\n",
      "2             LightGBM  0.978199   0.978303  0.978199  0.978198\n",
      "3  Logistic Regression  0.881388   0.882379  0.881388  0.881311\n",
      "4                  SVM  0.900061   0.901690  0.900061  0.899960\n",
      "5      Voting Ensemble  0.965800   0.965893  0.965800  0.965798\n",
      "6              Bagging  0.963696   0.963710  0.963696  0.963696\n",
      "7    Gradient Boosting  0.908774   0.909335  0.908774  0.908743\n",
      "8             Stacking  0.982388   0.982389  0.982388  0.982388\n",
      "9             AdaBoost  0.970659   0.970669  0.970659  0.970659\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Лучшие результаты**  \n",
    "- **Stacking** показал **наилучший результат** по всем метрикам (Accuracy = 0.982388, Precision = 0.982389, Recall = 0.982388, F1-Score = 0.982388).  \n",
    "  Ансамблирование методом Stacking эффективно использует сильные стороны отдельных моделей.  \n",
    "\n",
    "### 2. **Высокая производительность**  \n",
    "Следующие модели продемонстрировали близкие к оптимальным результаты:  \n",
    "- **LightGBM** (Accuracy = 0.978199, F1-Score = 0.978198)  \n",
    "- **XGBoost** (Accuracy = 0.973712, F1-Score = 0.973710)  \n",
    "- **Random Forest** (Accuracy = 0.970622, F1-Score = 0.970621)  \n",
    "- **AdaBoost** (Accuracy = 0.970659, F1-Score = 0.970659)\n",
    "\n",
    "**Вывод**: Ансамблирование методом бустинга (XGBoost, LightGBM, AdaBoost) и случайные леса (Random Forest) дают стабильные и высокие результаты."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
